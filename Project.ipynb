{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 6\n",
    "## 2301899156 - Muhammad Seitdha Fatah Roni\n",
    "## 2301900012 - Michael Hakkinen\n",
    "## 2301921881 - Rhamdany Ganio Teslatu\n",
    "## 2301924366 - Eric Wijayanto Wirawan\n",
    "## 2301934146 - Michael Rufi Tallaut Rongkos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to load the dataset\n",
    "def Load_Dataset(file_Name):\n",
    "    dataset = pd.read_csv(file_Name, encoding=\"ISO-8859-1\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Create a function to delete unused data\n",
    "def Delete_Unused_Data(dataset, column):\n",
    "    dataset.drop(column, axis = 1, inplace = True)\n",
    "\n",
    "    # delete nan\n",
    "    dataset = dataset.dropna()\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Create a function to preprocess the data\n",
    "def Preprocess_Data(data):\n",
    "    # Delete all symbol from the data\n",
    "    data = data.str.replace(\"@[\\w]*\", \"\")\n",
    "\n",
    "    # Change all whitespace in the data to space\n",
    "    data = data.str.replace(\"[^a-zA-Z0-9]\", \" \")\n",
    "\n",
    "    # Tokenize the data\n",
    "    text = data.apply(lambda x: x.split())\n",
    "\n",
    "    # Remove stopword from the data\n",
    "    stop_Words = stopwords.words('english')\n",
    "    text = text.apply(lambda x:[word for word in x if not word in stop_Words])\n",
    "\n",
    "    # Perform stemming on the data\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    text = text.apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "    # De-tokenize the data\n",
    "    processed = []\n",
    "    for i in text:\n",
    "        sentence = \"\"\n",
    "        for s in i:\n",
    "            sentence = sentence + str(s) + \" \"\n",
    "        processed.append(sentence)\n",
    "\n",
    "    return processed\n",
    "\n",
    "# Create a function to convert the label to number\n",
    "def Change_Label(label):\n",
    "    if label == 'neutral':\n",
    "        return 1\n",
    "    elif label == 'positive':\n",
    "        return 0\n",
    "    elif label == 'negative':\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class count\n",
      "neutral     11118\n",
      "positive     8582\n",
      "negative     7781\n",
      "Name: sentiment, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>Age of User</th>\n",
       "      <th>Country</th>\n",
       "      <th>Population -2020</th>\n",
       "      <th>Land Area (Km²)</th>\n",
       "      <th>Density (P/Km²)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>38928346</td>\n",
       "      <td>652860.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Albania</td>\n",
       "      <td>2877797</td>\n",
       "      <td>27400.0</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>43851044</td>\n",
       "      <td>2381740.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>77265</td>\n",
       "      <td>470.0</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Angola</td>\n",
       "      <td>32866272</td>\n",
       "      <td>1246700.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28b57f3990</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>night</td>\n",
       "      <td>70-100</td>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "      <td>97929</td>\n",
       "      <td>440.0</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>fun</td>\n",
       "      <td>positive</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>45195774</td>\n",
       "      <td>2736690.0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50e14c0bb8</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>neutral</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Armenia</td>\n",
       "      <td>2963243</td>\n",
       "      <td>28470.0</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e050245fbd</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>neutral</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Australia</td>\n",
       "      <td>25499884</td>\n",
       "      <td>7682300.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fc2cbefa9d</td>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n",
       "      <td>Wow... u just became cooler.</td>\n",
       "      <td>positive</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Austria</td>\n",
       "      <td>9006398</td>\n",
       "      <td>82400.0</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "5  28b57f3990  http://www.dothebouncy.com/smf - some shameles...   \n",
       "6  6e0c6d75b1  2am feedings for the baby are fun when he is a...   \n",
       "7  50e14c0bb8                                         Soooo high   \n",
       "8  e050245fbd                                        Both of you   \n",
       "9  fc2cbefa9d   Journey!? Wow... u just became cooler.  hehe....   \n",
       "\n",
       "                                       selected_text sentiment Time of Tweet  \\\n",
       "0                I`d have responded, if I were going   neutral       morning   \n",
       "1                                           Sooo SAD  negative          noon   \n",
       "2                                        bullying me  negative         night   \n",
       "3                                     leave me alone  negative       morning   \n",
       "4                                      Sons of ****,  negative          noon   \n",
       "5  http://www.dothebouncy.com/smf - some shameles...   neutral         night   \n",
       "6                                                fun  positive       morning   \n",
       "7                                         Soooo high   neutral          noon   \n",
       "8                                        Both of you   neutral         night   \n",
       "9                       Wow... u just became cooler.  positive       morning   \n",
       "\n",
       "  Age of User              Country  Population -2020  Land Area (Km²)  \\\n",
       "0        0-20          Afghanistan          38928346         652860.0   \n",
       "1       21-30              Albania           2877797          27400.0   \n",
       "2       31-45              Algeria          43851044        2381740.0   \n",
       "3       46-60              Andorra             77265            470.0   \n",
       "4       60-70               Angola          32866272        1246700.0   \n",
       "5      70-100  Antigua and Barbuda             97929            440.0   \n",
       "6        0-20            Argentina          45195774        2736690.0   \n",
       "7       21-30              Armenia           2963243          28470.0   \n",
       "8       31-45            Australia          25499884        7682300.0   \n",
       "9       46-60              Austria           9006398          82400.0   \n",
       "\n",
       "   Density (P/Km²)  \n",
       "0               60  \n",
       "1              105  \n",
       "2               18  \n",
       "3              164  \n",
       "4               26  \n",
       "5              223  \n",
       "6               17  \n",
       "7              104  \n",
       "8                3  \n",
       "9              109  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the training dataset\n",
    "dataset_Train = Load_Dataset(\"train.csv\")\n",
    "\n",
    "# Print the value count of each class in the dataset\n",
    "print(\"Class count\")\n",
    "print(dataset_Train['sentiment'].value_counts())\n",
    "\n",
    "# Print the first 10 data from the dataset\n",
    "dataset_Train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unused data from the dataset\n",
    "dataset_Train = Delete_Unused_Data(dataset_Train, ['textID', 'selected_text','Time of Tweet', 'Age of User', 'Country', 'Population -2020', 'Land Area (Km²)', 'Density (P/Km²)'])\n",
    "\n",
    "# Preprocess the data\n",
    "train_Data = Preprocess_Data(dataset_Train['text'])\n",
    "\n",
    "# Change the label to integer\n",
    "train_Target = dataset_Train['sentiment'].apply(lambda x: Change_Label(x))\n",
    "\n",
    "# Create a list that store the label\n",
    "label = [\n",
    "    \"positive\",\n",
    "    \"neutral\",\n",
    "    \"negative\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform TF-IDF\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,3)).fit(train_Data)\n",
    "train_Feature = vectorizer.transform(train_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Train:(19236, 300749)  Target Train:(19236,)\n",
      "Feature Validation:(5770, 300749)  Target Validation:(5770,)\n",
      "Feature Test:(2474, 300749)  Target Test:(2474,)\n"
     ]
    }
   ],
   "source": [
    "# Divide the data into 70% training set and 30% testing set\n",
    "feature_Train, feature_Test, target_Train, target_Test = train_test_split(train_Feature, train_Target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Divide the testing set into 70% validation set and 30% testing set\n",
    "feature_Validation, feature_Test, target_Validation, target_Test = train_test_split(feature_Test, target_Test, test_size=0.3, random_state=42)\n",
    "\n",
    "# Print the shape of each data\n",
    "print(f\"Feature Train:{feature_Train.shape}  Target Train:{target_Train.shape}\")\n",
    "print(f\"Feature Validation:{feature_Validation.shape}  Target Validation:{target_Validation.shape}\")\n",
    "print(f\"Feature Test:{feature_Test.shape}  Target Test:{target_Test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to calculate and print recall score, precision score and accuracy score\n",
    "def Print_Metric(model_Name, target_True, target_Predict):\n",
    "    # Calculate recall, precision and accuracy\n",
    "    recall = recall_score(target_True, target_Predict, average=\"macro\")\n",
    "    precision = precision_score(target_True, target_Predict, average=\"macro\")\n",
    "    accuracy = accuracy_score(target_True, target_Predict)\n",
    "\n",
    "    # Print recall, precision and accuracy\n",
    "    print(f\"Model {model_Name}\")\n",
    "    print(\"Recall: {:.3f}\".format(recall))\n",
    "    print(\"Precision: {:.3f}\".format(precision))\n",
    "    print(\"Accuracy: {:.3f}\".format(accuracy))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create KNN model\n",
    "model_KNN = KNeighborsClassifier(n_neighbors=7, algorithm=\"auto\", leaf_size=10, p=1, n_jobs=-1)\n",
    "\n",
    "# Train the model\n",
    "model_KNN.fit(feature_Train, target_Train)\n",
    "\n",
    "# Use the model to predict the testing set\n",
    "target_Predict_KNN = model_KNN.predict(feature_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "# Create Logistic Regression model\n",
    "model_LogisticRegression = LogisticRegression(max_iter = 200, penalty=\"l1\", C=5.0, solver='liblinear', class_weight='balanced')\n",
    "\n",
    "# Train the model\n",
    "model_LogisticRegression.fit(feature_Train, target_Train)\n",
    "\n",
    "# Use the model to predict the testing set\n",
    "target_Predict_LogisticRegression = model_LogisticRegression.predict(feature_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:1.06361\n",
      "[1]\tvalidation_0-mlogloss:1.03377\n",
      "[2]\tvalidation_0-mlogloss:1.00834\n",
      "[3]\tvalidation_0-mlogloss:0.98651\n",
      "[4]\tvalidation_0-mlogloss:0.96732\n",
      "[5]\tvalidation_0-mlogloss:0.95100\n",
      "[6]\tvalidation_0-mlogloss:0.93660\n",
      "[7]\tvalidation_0-mlogloss:0.92401\n",
      "[8]\tvalidation_0-mlogloss:0.91296\n",
      "[9]\tvalidation_0-mlogloss:0.90283\n",
      "[10]\tvalidation_0-mlogloss:0.89398\n",
      "[11]\tvalidation_0-mlogloss:0.88604\n",
      "[12]\tvalidation_0-mlogloss:0.87925\n",
      "[13]\tvalidation_0-mlogloss:0.87245\n",
      "[14]\tvalidation_0-mlogloss:0.86652\n",
      "[15]\tvalidation_0-mlogloss:0.86124\n",
      "[16]\tvalidation_0-mlogloss:0.85625\n",
      "[17]\tvalidation_0-mlogloss:0.85188\n",
      "[18]\tvalidation_0-mlogloss:0.84723\n",
      "[19]\tvalidation_0-mlogloss:0.84353\n",
      "[20]\tvalidation_0-mlogloss:0.84002\n",
      "[21]\tvalidation_0-mlogloss:0.83650\n",
      "[22]\tvalidation_0-mlogloss:0.83325\n",
      "[23]\tvalidation_0-mlogloss:0.83005\n",
      "[24]\tvalidation_0-mlogloss:0.82705\n",
      "[25]\tvalidation_0-mlogloss:0.82433\n",
      "[26]\tvalidation_0-mlogloss:0.82209\n",
      "[27]\tvalidation_0-mlogloss:0.81931\n",
      "[28]\tvalidation_0-mlogloss:0.81654\n",
      "[29]\tvalidation_0-mlogloss:0.81424\n",
      "[30]\tvalidation_0-mlogloss:0.81215\n",
      "[31]\tvalidation_0-mlogloss:0.81012\n",
      "[32]\tvalidation_0-mlogloss:0.80800\n",
      "[33]\tvalidation_0-mlogloss:0.80606\n",
      "[34]\tvalidation_0-mlogloss:0.80394\n",
      "[35]\tvalidation_0-mlogloss:0.80209\n",
      "[36]\tvalidation_0-mlogloss:0.80034\n",
      "[37]\tvalidation_0-mlogloss:0.79896\n",
      "[38]\tvalidation_0-mlogloss:0.79709\n",
      "[39]\tvalidation_0-mlogloss:0.79552\n",
      "[40]\tvalidation_0-mlogloss:0.79384\n",
      "[41]\tvalidation_0-mlogloss:0.79233\n",
      "[42]\tvalidation_0-mlogloss:0.79102\n",
      "[43]\tvalidation_0-mlogloss:0.78995\n",
      "[44]\tvalidation_0-mlogloss:0.78879\n",
      "[45]\tvalidation_0-mlogloss:0.78732\n",
      "[46]\tvalidation_0-mlogloss:0.78583\n",
      "[47]\tvalidation_0-mlogloss:0.78460\n",
      "[48]\tvalidation_0-mlogloss:0.78345\n",
      "[49]\tvalidation_0-mlogloss:0.78238\n",
      "[50]\tvalidation_0-mlogloss:0.78145\n",
      "[51]\tvalidation_0-mlogloss:0.78050\n",
      "[52]\tvalidation_0-mlogloss:0.77955\n",
      "[53]\tvalidation_0-mlogloss:0.77860\n",
      "[54]\tvalidation_0-mlogloss:0.77758\n",
      "[55]\tvalidation_0-mlogloss:0.77682\n",
      "[56]\tvalidation_0-mlogloss:0.77592\n",
      "[57]\tvalidation_0-mlogloss:0.77511\n",
      "[58]\tvalidation_0-mlogloss:0.77418\n",
      "[59]\tvalidation_0-mlogloss:0.77333\n",
      "[60]\tvalidation_0-mlogloss:0.77265\n",
      "[61]\tvalidation_0-mlogloss:0.77200\n",
      "[62]\tvalidation_0-mlogloss:0.77114\n",
      "[63]\tvalidation_0-mlogloss:0.77030\n",
      "[64]\tvalidation_0-mlogloss:0.76974\n",
      "[65]\tvalidation_0-mlogloss:0.76909\n",
      "[66]\tvalidation_0-mlogloss:0.76858\n",
      "[67]\tvalidation_0-mlogloss:0.76787\n",
      "[68]\tvalidation_0-mlogloss:0.76730\n",
      "[69]\tvalidation_0-mlogloss:0.76654\n",
      "[70]\tvalidation_0-mlogloss:0.76598\n",
      "[71]\tvalidation_0-mlogloss:0.76560\n",
      "[72]\tvalidation_0-mlogloss:0.76519\n",
      "[73]\tvalidation_0-mlogloss:0.76487\n",
      "[74]\tvalidation_0-mlogloss:0.76445\n",
      "[75]\tvalidation_0-mlogloss:0.76376\n",
      "[76]\tvalidation_0-mlogloss:0.76327\n",
      "[77]\tvalidation_0-mlogloss:0.76262\n",
      "[78]\tvalidation_0-mlogloss:0.76210\n",
      "[79]\tvalidation_0-mlogloss:0.76190\n",
      "[80]\tvalidation_0-mlogloss:0.76160\n",
      "[81]\tvalidation_0-mlogloss:0.76116\n",
      "[82]\tvalidation_0-mlogloss:0.76063\n",
      "[83]\tvalidation_0-mlogloss:0.76017\n",
      "[84]\tvalidation_0-mlogloss:0.75985\n",
      "[85]\tvalidation_0-mlogloss:0.75941\n",
      "[86]\tvalidation_0-mlogloss:0.75914\n",
      "[87]\tvalidation_0-mlogloss:0.75879\n",
      "[88]\tvalidation_0-mlogloss:0.75842\n",
      "[89]\tvalidation_0-mlogloss:0.75799\n",
      "[90]\tvalidation_0-mlogloss:0.75774\n",
      "[91]\tvalidation_0-mlogloss:0.75734\n",
      "[92]\tvalidation_0-mlogloss:0.75706\n",
      "[93]\tvalidation_0-mlogloss:0.75676\n",
      "[94]\tvalidation_0-mlogloss:0.75633\n",
      "[95]\tvalidation_0-mlogloss:0.75585\n",
      "[96]\tvalidation_0-mlogloss:0.75563\n",
      "[97]\tvalidation_0-mlogloss:0.75532\n",
      "[98]\tvalidation_0-mlogloss:0.75504\n",
      "[99]\tvalidation_0-mlogloss:0.75465\n",
      "[100]\tvalidation_0-mlogloss:0.75430\n",
      "[101]\tvalidation_0-mlogloss:0.75409\n",
      "[102]\tvalidation_0-mlogloss:0.75364\n",
      "[103]\tvalidation_0-mlogloss:0.75357\n",
      "[104]\tvalidation_0-mlogloss:0.75324\n",
      "[105]\tvalidation_0-mlogloss:0.75320\n",
      "[106]\tvalidation_0-mlogloss:0.75294\n",
      "[107]\tvalidation_0-mlogloss:0.75280\n",
      "[108]\tvalidation_0-mlogloss:0.75256\n",
      "[109]\tvalidation_0-mlogloss:0.75236\n",
      "[110]\tvalidation_0-mlogloss:0.75223\n",
      "[111]\tvalidation_0-mlogloss:0.75206\n",
      "[112]\tvalidation_0-mlogloss:0.75180\n",
      "[113]\tvalidation_0-mlogloss:0.75152\n",
      "[114]\tvalidation_0-mlogloss:0.75119\n",
      "[115]\tvalidation_0-mlogloss:0.75094\n",
      "[116]\tvalidation_0-mlogloss:0.75078\n",
      "[117]\tvalidation_0-mlogloss:0.75059\n",
      "[118]\tvalidation_0-mlogloss:0.75043\n",
      "[119]\tvalidation_0-mlogloss:0.75012\n",
      "[120]\tvalidation_0-mlogloss:0.74997\n",
      "[121]\tvalidation_0-mlogloss:0.74983\n",
      "[122]\tvalidation_0-mlogloss:0.74968\n",
      "[123]\tvalidation_0-mlogloss:0.74961\n",
      "[124]\tvalidation_0-mlogloss:0.74936\n",
      "[125]\tvalidation_0-mlogloss:0.74911\n",
      "[126]\tvalidation_0-mlogloss:0.74894\n",
      "[127]\tvalidation_0-mlogloss:0.74875\n",
      "[128]\tvalidation_0-mlogloss:0.74854\n",
      "[129]\tvalidation_0-mlogloss:0.74848\n"
     ]
    }
   ],
   "source": [
    "# Create XGBoost model\n",
    "model_XGBoost = XGBClassifier(objective=\"multi:softprob\", use_label_encoder=False, eval_metric='mlogloss', booster='gbtree', learning_rate=0.1, max_depth=20, min_child_weight=10, n_estimators=130)\n",
    "\n",
    "# Train the model\n",
    "model_XGBoost.fit(feature_Train, target_Train, eval_set=[(feature_Validation, target_Validation)], verbose=True)\n",
    "\n",
    "# Use the model to predict the testing set \n",
    "target_Predict_XGBoost = model_XGBoost.predict(feature_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LogisticRegression\n",
      "Recall: 0.707\n",
      "Precision: 0.720\n",
      "Accuracy: 0.712\n",
      "\n",
      "Model KNN\n",
      "Recall: 0.342\n",
      "Precision: 0.784\n",
      "Accuracy: 0.413\n",
      "\n",
      "Model XGBClassifier\n",
      "Recall: 0.684\n",
      "Precision: 0.724\n",
      "Accuracy: 0.700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the result\n",
    "Print_Metric(\"LogisticRegression\", target_Test, target_Predict_LogisticRegression)\n",
    "Print_Metric(\"KNN\", target_Test, target_Predict_KNN)\n",
    "Print_Metric(\"XGBClassifier\", target_Test, target_Predict_XGBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Because Logistic Regression have the most highest value, Logistic Regression model will be used to predict the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "dataset_Test = Load_Dataset(\"test.csv\")\n",
    "\n",
    "# Delete unused data from test dataset\n",
    "dataset_Test = Delete_Unused_Data(dataset_Test, ['textID','Time of Tweet', 'Age of User', 'Country', 'Population -2020', 'Land Area (Km²)', 'Density (P/Km²)'])\n",
    "\n",
    "# Preprocess the data\n",
    "test_Data = Preprocess_Data(dataset_Test['text'])\n",
    "\n",
    "# Change the label to integer\n",
    "test_Target = dataset_Test['sentiment'].apply(lambda x: Change_Label(x))\n",
    "\n",
    "# Use tf idf\n",
    "test_Data = vectorizer.transform(test_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LogisticRegression\n",
      "Recall: 0.716\n",
      "Precision: 0.701\n",
      "Accuracy: 0.702\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the model to predict the test dataset\n",
    "target_Predict_Test = model_LogisticRegression.predict(test_Data)\n",
    "\n",
    "# Print the recall, precision and accuracy\n",
    "Print_Metric(\"LogisticRegression\", test_Target, target_Predict_Test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "380030d1298d5a27518acca789ff38fe82bbf2e68b73263de6a6bf23efb7704c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
